{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# =========================\n","# Import necessary dependencies\n","# =========================\n","import os, re, json\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from IPython.display import display\n","\n","import time, urllib.parse, requests, unicodedata\n","\n","from collections import Counter, defaultdict\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from scipy.sparse import csr_matrix\n","\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","# -----------------------------\n","# Configuration\n","# -----------------------------\n","DATA_PATH = \"medical_abstract.csv\"   # <-- change if needed\n","TEXT_COL  = \"Medical_Abstract\"\n","LABEL_COL = \"Category_Name\"\n","\n","TABLE_DIR = \"result/tables\"\n","PLOT_DIR  = \"result/plots\"\n","SEP = \"=\" * 80\n","\n","os.makedirs(TABLE_DIR, exist_ok=True)\n","os.makedirs(PLOT_DIR, exist_ok=True)\n","\n","# Plot aesthetics & dimensions\n","plt.rcParams[\"figure.figsize\"] = (9, 5)\n","plt.rcParams[\"axes.titlesize\"] = 14\n","plt.rcParams[\"axes.labelsize\"] = 12\n","plt.rcParams[\"xtick.labelsize\"] = 10\n","plt.rcParams[\"ytick.labelsize\"] = 10\n","plt.rcParams[\"legend.fontsize\"] = 10\n","plt.rcParams[\"grid.alpha\"] = 0.25\n","plt.rcParams[\"axes.prop_cycle\"] = plt.cycler(color=[\n","    \"#4E79A7\", \"#F28E2B\", \"#E15759\", \"#76B7B2\", \"#59A14F\",\n","    \"#EDC948\", \"#B07AA1\", \"#FF9DA7\", \"#9C755F\", \"#BAB0AC\"\n","])\n","\n","# -----------------------------\n","#Customized Functions\n","# -----------------------------\n","def _round_df(df: pd.DataFrame, ndigits: int = 3) -> pd.DataFrame:\n","    out = df.copy()\n","    for c in out.columns:\n","        if pd.api.types.is_numeric_dtype(out[c]):\n","            out[c] = out[c].astype(float).round(ndigits)\n","    return out\n","\n","def _show_and_save(fig, out_path_png):\n","    plt.tight_layout()\n","    plt.show()\n","    fig.savefig(out_path_png, dpi=1200, bbox_inches=\"tight\")\n","    print(SEP)\n","\n","# -----------------------------\n","# Load\n","# -----------------------------\n","df = pd.read_csv(DATA_PATH)\n","df[TEXT_COL] = df[TEXT_COL].astype(str)\n","\n","# -----------------------------\n","# Null checks\n","# -----------------------------\n","n_rows = len(df)\n","null_df = df.isna().sum().to_frame(\"null_count\")\n","null_df[\"null_frac_%\"] = (null_df[\"null_count\"] / n_rows * 100).round(3)\n","display(_round_df(null_df.reset_index().rename(columns={\"index\": \"column\"})))\n","print(SEP)\n","\n","# -----------------------------\n","# Duplicate checks\n","# -----------------------------\n","dup_rows_mask = df.duplicated(keep=False)  # exact row duplicates\n","dup_text_mask = df[TEXT_COL].duplicated(keep=False)  # identical text, regardless of label\n","\n","dup_summary = pd.DataFrame({\n","    \"metric\": [\"duplicate_rows\", \"duplicate_texts\"],\n","    \"count\": [int(dup_rows_mask.sum()), int(dup_text_mask.sum())],\n","    \"frac_%\": [round(dup_rows_mask.mean() * 100, 3), round(dup_text_mask.mean() * 100, 3)]\n","})\n","display(_round_df(dup_summary))\n","print(SEP)\n","\n","# -----------------------------\n","# Text duplicates across categories (cross-label)\n","# -----------------------------\n","# Group identical texts; counting distinct labels; keeping those having >1 label\n","g_text = df.groupby(TEXT_COL)\n","group_size = g_text.size().rename(\"group_size\")\n","n_labels = g_text[LABEL_COL].nunique().rename(\"n_labels\")\n","\n","meta = pd.concat([group_size, n_labels], axis=1).reset_index()\n","cross_label = meta[(meta[\"group_size\"] > 1) & (meta[\"n_labels\"] > 1)].copy()\n","\n","# Building a compact table with a short snippet and counts per label\n","def _label_counts_dict(grp):\n","    return grp[LABEL_COL].value_counts().to_dict()\n","\n","lbl_counts = g_text.apply(_label_counts_dict).rename(\"label_counts\").reset_index()\n","cross_label = cross_label.merge(lbl_counts, on=TEXT_COL, how=\"left\")\n","cross_label[\"text_snippet\"] = cross_label[TEXT_COL].str.slice(0, 160)\n","\n","# Displaying top 15 largest cross-label groups\n","cross_label_view = cross_label.sort_values(\"group_size\", ascending=False)[\n","    [\"text_snippet\", \"group_size\", \"n_labels\", \"label_counts\"]\n","].head(15)\n","display(cross_label_view)\n","print(SEP)\n","\n","# -----------------------------\n","# Label distribution\n","# -----------------------------\n","label_counts = (\n","    df[LABEL_COL].value_counts()\n","      .rename_axis(\"Category\")\n","      .to_frame(\"Count\")\n","      .reset_index()\n",")\n","label_counts[\"Frac_%\"] = (label_counts[\"Count\"] / n_rows * 100).round(3)\n","display(_round_df(label_counts))\n","print(SEP)\n","\n","# Save table\n","_round_df(label_counts).to_csv(os.path.join(TABLE_DIR, \"label_distribution.csv\"), index=False)\n","\n","# -----------------------------\n","# Length stats & overview\n","# -----------------------------\n","df[\"_char_len\"] = df[TEXT_COL].str.len()\n","df[\"_word_len\"] = df[TEXT_COL].str.split().map(len)\n","\n","length_stats = pd.DataFrame([{\n","    \"count\": len(df),\n","    \"char_mean\": df[\"_char_len\"].mean(),\n","    \"char_std\": df[\"_char_len\"].std(),\n","    \"char_min\": df[\"_char_len\"].min(),\n","    \"char_25%\": df[\"_char_len\"].quantile(0.25),\n","    \"char_50%\": df[\"_char_len\"].quantile(0.50),\n","    \"char_75%\": df[\"_char_len\"].quantile(0.75),\n","    \"char_max\": df[\"_char_len\"].max(),\n","    \"word_mean\": df[\"_word_len\"].mean(),\n","    \"word_std\": df[\"_word_len\"].std(),\n","    \"word_min\": df[\"_word_len\"].min(),\n","    \"word_25%\": df[\"_word_len\"].quantile(0.25),\n","    \"word_50%\": df[\"_word_len\"].quantile(0.50),\n","    \"word_75%\": df[\"_word_len\"].quantile(0.75),\n","    \"word_max\": df[\"_word_len\"].max(),\n","}])\n","display(_round_df(length_stats))\n","print(SEP)\n","\n","# Overview metrics\n","short_thresh_words = 30\n","long_thresh_words  = int(np.ceil(df[\"_word_len\"].quantile(0.99)))\n","df[\"_is_short\"] = df[\"_word_len\"] < short_thresh_words\n","df[\"_is_long\"]  = df[\"_word_len\"] >= long_thresh_words\n","\n","rows_in_crosslabel_groups = int(\n","    df[TEXT_COL].isin(set(cross_label[TEXT_COL].values)).sum()\n",")\n","\n","overview = pd.DataFrame({\n","    \"metric\": [\n","        \"n_rows\", \"n_cols\",\n","        \"duplicate_rows\", \"duplicate_texts\",\n","        \"crosslabel_groups\", \"rows_in_crosslabel_groups\",\n","        \"short_texts_<30w\", f\"very_long_texts_>=P99w({long_thresh_words})\"\n","    ],\n","    \"value\": [\n","        len(df), df.shape[1],\n","        int(dup_rows_mask.sum()), int(dup_text_mask.sum()),\n","        int(len(cross_label)), rows_in_crosslabel_groups,\n","        int(df[\"_is_short\"].sum()), int(df[\"_is_long\"].sum())\n","    ]\n","})\n","display(_round_df(overview))\n","print(SEP)\n","\n","# Save tables\n","_round_df(length_stats).to_csv(os.path.join(TABLE_DIR, \"text_length_stats.csv\"), index=False)\n","_round_df(overview).to_csv(os.path.join(TABLE_DIR, \"dataset_overview.csv\"), index=False)\n","\n","# -----------------------------\n","# Plot: Category distribution\n","# -----------------------------\n","fig = plt.figure()\n","cats = label_counts[\"Category\"].tolist()\n","vals = label_counts[\"Count\"].tolist()\n","bars = plt.bar(range(len(cats)), vals)\n","plt.xticks(range(len(cats)), cats, rotation=30, ha='right')\n","plt.ylabel(\"Count\")\n","plt.title(f\"Category Distribution (N = {len(df)})\")\n","for i, b in enumerate(bars):\n","    h = b.get_height()\n","    pct = vals[i] / len(df) * 100\n","    plt.text(b.get_x() + b.get_width()/2, h, f\"{int(h)} ({pct:.3f}%)\",\n","             ha='center', va='bottom', fontsize=9)\n","plt.grid(True, axis='y')\n","_show_and_save(fig, os.path.join(PLOT_DIR, \"category_distribution.png\"))\n","\n","# -----------------------------\n","# Plot: Word-count boxplot by category\n","# -----------------------------\n","fig2 = plt.figure()\n","order_by_median = (\n","    df.groupby(LABEL_COL)[\"_word_len\"]\n","      .median()\n","      .sort_values(ascending=False)\n","      .index.tolist()\n",")\n","data_per_cat = [df[df[LABEL_COL] == c][\"_word_len\"].values for c in order_by_median]\n","plt.boxplot(data_per_cat, labels=order_by_median, showmeans=True)\n","plt.xticks(rotation=30, ha='right')\n","plt.ylabel(\"Word Count\")\n","plt.title(\"Word Count by Category (with means)\")\n","plt.grid(True, axis='y')\n","_show_and_save(fig2, os.path.join(PLOT_DIR, \"word_count_by_category_boxplot.png\"))"],"metadata":{"id":"4gVh-4qymgOA","executionInfo":{"status":"ok","timestamp":1760832611365,"user_tz":-60,"elapsed":40,"user":{"displayName":"Idris Babalola","userId":"12686428794913987650"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# ----- Displaying and exporting the identified text level duplicates ------\n","duplicates = df[df.duplicated(subset=[TEXT_COL], keep=False)]\n","sorted_duplicates = duplicates.sort_values(by=[TEXT_COL, LABEL_COL])\n","sorted_duplicates.head(20)\n","sorted_duplicates = sorted_duplicates[['Category', 'Medical_Abstract', 'Category_Name', '_char_len', '_word_len']]\n","sorted_duplicates.to_csv(os.path.join(TABLE_DIR, \"sorted_duplicates.csv\"), index=False)\n","sorted_duplicates.head(20)"],"metadata":{"id":"p6sNGNPeaPzC","executionInfo":{"status":"ok","timestamp":1760832641959,"user_tz":-60,"elapsed":39,"user":{"displayName":"Idris Babalola","userId":"12686428794913987650"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# =============================================================================\n","# Conservative deduplication (i.e not retaining any of the duplicate instance)\n","# =============================================================================\n","\n","def normalize_unicode_ws(s: str) -> str:\n","    s = unicodedata.normalize(\"NFKC\", s)\n","    s = re.sub(r\"\\s+\", \" \", s).strip()\n","    return s\n","\n","# --- Conservative dedup (drop whole duplicate groups) ---\n","# Build normalized key (keep text intact; this is only for duplicate detection)\n","df[\"_norm_key\"] = df[TEXT_COL].map(normalize_unicode_ws)\n","\n","key_counts = df[\"_norm_key\"].value_counts()\n","unique_mask = df[\"_norm_key\"].map(key_counts) == 1\n","df_after = df[unique_mask].copy()\n","\n","# Drop ultra-short AFTER dedup\n","df_after[\"_word_len\"] = df_after[TEXT_COL].str.split().map(len)\n","df_after = df_after[df_after[\"_word_len\"] >= 30].copy()\n","\n","# --- AFTER label distribution & overview ---\n","n_after = len(df_after)\n","label_counts_after = (\n","    df_after[LABEL_COL].value_counts()\n","      .rename_axis(\"Category\")\n","      .to_frame(\"Count\")\n","      .reset_index()\n",")\n","label_counts_after[\"Frac_%\"] = (label_counts_after[\"Count\"] / n_after * 100).round(3)\n","display(_round_df(label_counts_after))\n","\n","overview_after = pd.DataFrame({\n","    \"metric\": [\"n_rows_after\", \"n_cols\", \"dropped_duplicate_groups\", \"dropped_ultra_short_<30w\"],\n","    \"value\": [\n","        int(n_after),\n","        int(df_after.shape[1]),\n","        int((key_counts > 1).sum()),\n","        int((df[\"_norm_key\"].map(key_counts) == 1).sum() - n_after)  # uniques before short-drop minus kept\n","    ]\n","})\n","display(_round_df(overview_after))\n","\n","# save tables\n","_round_df(label_counts_after).to_csv(os.path.join(TABLE_DIR, \"label_distribution_after_conservative_dedup.csv\"), index=False)\n","_round_df(overview_after).to_csv(os.path.join(TABLE_DIR, \"dataset_overview_after_conservative_dedup.csv\"), index=False)"],"metadata":{"id":"ujAyxWdiciaP","executionInfo":{"status":"ok","timestamp":1760832646164,"user_tz":-60,"elapsed":62,"user":{"displayName":"Idris Babalola","userId":"12686428794913987650"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# ====================================================\n","# Plotting before Vs after to inspect interim dataset\n","# ====================================================\n","\n","def plot_before_after_grouped(dist_before: pd.DataFrame, n_before: int,\n","                              dist_after: pd.DataFrame, n_after: int,\n","                              title: str, outfile: str,\n","                              sort_by: str = \"after\"):\n","    # union of categories\n","    cats = sorted(set(dist_before[\"Category\"]).union(set(dist_after[\"Category\"])))\n","    b_map = dict(zip(dist_before[\"Category\"], dist_before[\"Count\"]))\n","    a_map = dict(zip(dist_after[\"Category\"],  dist_after[\"Count\"]))\n","    before = np.array([b_map.get(c, 0) for c in cats], dtype=int)\n","    after  = np.array([a_map.get(c, 0) for c in cats], dtype=int)\n","\n","    # sorting\n","    if sort_by.lower() == \"before\":\n","        order = np.argsort(-before)\n","    elif sort_by.lower() == \"total\":\n","        order = np.argsort(-(before + after))\n","    else:  # \"after\"\n","        order = np.argsort(-after)\n","    cats_sorted   = [cats[i] for i in order]\n","    before_sorted = before[order]\n","    after_sorted  = after[order]\n","\n","    # y positions and bar geometry\n","    y = np.arange(len(cats_sorted))\n","    h = 0.38 #bar thickness\n","    y_before = y - h/2\n","    y_after  = y + h/2\n","\n","    # figure\n","    fig, ax = plt.subplots(figsize=(9.5, 4.5))\n","    color_before = \"#4E79A7\"\n","    color_after  = \"#28A197\"\n","\n","    # bars (horizontal)\n","    b_plot = ax.barh(y_before, before_sorted, height=h, label=\"Before\", color=color_before)\n","    a_plot = ax.barh(y_after,  after_sorted,  height=h, label=\"After\",  color=color_after)\n","\n","    # axes & title\n","    ax.set_yticks(y)\n","    ax.set_yticklabels(cats_sorted)\n","    ax.invert_yaxis()\n","    ax.set_xlabel(\"Count\")\n","    ax.set_ylabel(\"\")\n","    ax.grid(True, axis=\"x\", alpha=0.25)\n","\n","    # combined title with sample sizes\n","    ax.set_title(f\"{title}\\nBefore (N={n_before}) | After (N={n_after})\", pad=12)\n","\n","    # annotations (count and % at the *front* of bars)\n","    max_val = max(before_sorted.max() if len(before_sorted) else 0,\n","                  after_sorted.max()  if len(after_sorted)  else 0)\n","    x_pad = max(1, int(max_val * 0.01))\n","\n","    for rect, cnt in zip(b_plot, before_sorted):\n","        pct = (cnt / n_before * 100) if n_before else 0.0\n","        x = rect.get_width()\n","        ytxt = rect.get_y() + rect.get_height()/2\n","        ax.text(x + x_pad, ytxt, f\"{int(cnt)} ({pct:.3f}%)\",\n","                va=\"center\", ha=\"left\", fontsize=9)\n","\n","    for rect, cnt in zip(a_plot, after_sorted):\n","        pct = (cnt / n_after * 100) if n_after else 0.0\n","        x = rect.get_width()\n","        ytxt = rect.get_y() + rect.get_height()/2\n","        ax.text(x + x_pad, ytxt, f\"{int(cnt)} ({pct:.3f}%)\",\n","                va=\"center\", ha=\"left\", fontsize=9)\n","\n","    ax.set_xlim(0, max_val * 1.20 + x_pad)\n","    ax.legend(loc=\"center left\", bbox_to_anchor=(1.02, 0.5), frameon=True)\n","\n","    plt.tight_layout()\n","    _show_and_save(fig, os.path.join(PLOT_DIR, outfile))\n","\n","plot_before_after_grouped(\n","    label_counts_before := label_counts.copy(), len(df),\n","    label_counts_after, n_after,\n","    title=\"Category Distribution: Conservative Deduplication + Drop <30 tokens\",\n","    outfile=\"category_distribution_grouped_before_after.png\",\n","    sort_by=\"after\")"],"metadata":{"id":"qeNBBZzcyZd2","executionInfo":{"status":"ok","timestamp":1760832664136,"user_tz":-60,"elapsed":28,"user":{"displayName":"Idris Babalola","userId":"12686428794913987650"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# ==================\n","# PubMed spot-check\n","# ==================\n","\n","# --- Config ---\n","ENABLE_PUBMED_CHECK = True\n","PUBMED_SAMPLE_N = 100\n","SNIPPET_WORDS   = 12\n","PUBMED_BASE     = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n","PUBMED_DB       = \"pubmed\"\n","PUBMED_RETMODE  = \"json\"\n","PUBMED_RETMAX   = 1\n","PUBMED_SLEEP    = 0.50 # ~2 req/s\n","PUBMED_API_KEY  = None # put your NCBI API key here if you have one\n","NCBI_TOOL       = \"reliability-check\"\n","NCBI_EMAIL      = \"your_email@example.com\"  # recommended by NCBI\n","\n","# --- Building stopword for this block ---\n","STOPWORDS = {\n","    \"the\",\"a\",\"an\",\"of\",\"and\",\"or\",\"to\",\"in\",\"on\",\"for\",\"with\",\"by\",\"as\",\"at\",\"from\",\n","    \"is\",\"are\",\"was\",\"were\",\"be\",\"being\",\"been\",\"that\",\"this\",\"these\",\"those\",\"it\",\"its\",\n","    \"we\",\"they\",\"their\",\"our\",\"you\",\"your\",\"he\",\"she\",\"his\",\"her\",\"which\",\"who\",\"whom\"\n","}\n","\n","def _net_ping():\n","    try:\n","        r = requests.get(\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/einfo.fcgi?db=pubmed\",\n","                         timeout=10, headers={\"User-Agent\":\"qa-spotcheck/1.0\"})\n","        print(\"NCBI eutils reachable, status:\", r.status_code)\n","    except Exception as e:\n","        print(\"NCBI eutils NOT reachable:\", repr(e))\n","\n","# --- Whitespace & Unicode normalization ---\n","def _clean_ws(s: str) -> str:\n","    s = unicodedata.normalize(\"NFKC\", s)\n","    s = re.sub(r\"\\s+\", \" \", s).strip()\n","    return s\n","\n","# --- First-N words for snippet creation ---\n","def _first_n_words(s, n=SNIPPET_WORDS):\n","    return \" \".join(s.split()[:n])\n","\n","# --- Construct PubMed AND-term query from a snippet ---\n","def _and_terms(snippet, k=6):\n","    toks = re.findall(r\"[A-Za-z0-9]+\", snippet.lower())\n","    toks = [t for t in toks if t not in STOPWORDS]\n","    toks = toks[:k]\n","    if not toks:\n","        return None\n","    return \" AND \".join(f\"{t}[Title/Abstract]\" for t in toks)\n","\n"," # --- eSearch wrapper (NCBI Entrez) ---\n","def _esearch(term, retmax=1):\n","    params = {\n","        \"db\": PUBMED_DB,\n","        \"retmode\": PUBMED_RETMODE,\n","        \"retmax\": str(retmax),\n","        \"term\": term,\n","        \"tool\": NCBI_TOOL,\n","        \"email\": NCBI_EMAIL\n","    }\n","    if PUBMED_API_KEY:\n","        params[\"api_key\"] = PUBMED_API_KEY\n","    url = PUBMED_BASE + \"?\" + urllib.parse.urlencode(params, safe='\"[] ')\n","    r = requests.get(url, headers={\"User-Agent\": \"qa-spotcheck/1.0\"}, timeout=20)\n","    if r.status_code != 200:\n","        return r.status_code, []\n","    data = r.json()\n","    return r.status_code, data.get(\"esearchresult\", {}).get(\"idlist\", [])\n","\n","# --- Matching strategy cascade for a snippet ---\n","def _try_match(snippet, debug=False):\n","    s = _clean_ws(snippet).replace('\"', \"\")\n","    # Tier 1: exact 12 words in Title/Abstract\n","    t1 = f\"\\\"{s}\\\"[Title/Abstract]\"\n","    code, ids = _esearch(t1)\n","    if debug: print(\"exact12:\", code, \"ids:\", len(ids))\n","    if ids: return True, ids[0], \"exact12\", code\n","\n","    # Tier 2: exact 8 words in Title/Abstract\n","    w = s.split()\n","    if len(w) > 8:\n","        s8 = \" \".join(w[:8])\n","        t2 = f\"\\\"{s8}\\\"[Title/Abstract]\"\n","        code, ids = _esearch(t2)\n","        if debug: print(\"exact8:\", code, \"ids:\", len(ids))\n","        if ids: return True, ids[0], \"exact8\", code\n","\n","    # Tier 3: AND of 6 content terms\n","    t3 = _and_terms(s, k=6)\n","    if t3:\n","        code, ids = _esearch(t3)\n","        if debug: print(\"and6:\", code, \"ids:\", len(ids), \"|\", t3[:120])\n","        if ids: return True, ids[0], \"and6\", code\n","\n","    # Tier 4: exact phrase without field tag (fallback)\n","    t4 = f\"\\\"{s}\\\"\"\n","    code, ids = _esearch(t4)\n","    if debug: print(\"nofield:\", code, \"ids:\", len(ids))\n","    if ids: return True, ids[0], \"nofield\", code\n","\n","    return False, \"\", \"nomatch\", code\n","\n","# --- Run spot-check on the deduplicated data ---\n","if not ENABLE_PUBMED_CHECK:\n","    print(\"PubMed spot-check disabled (ENABLE_PUBMED_CHECK=False).\")\n","else:\n","    _net_ping()\n","\n","    if \"df_after\" not in globals() or df_after.empty:\n","        print(\"No df_after found or it's empty. Did you run the dedup cell?\")\n","    else:\n","        sample_n = min(PUBMED_SAMPLE_N, len(df_after))\n","        sample_df = df_after.sample(n=sample_n, random_state=42)[[TEXT_COL, LABEL_COL]].reset_index(drop=True)\n","        sample_df[\"snippet\"] = sample_df[TEXT_COL].apply(lambda s: _first_n_words(s, n=SNIPPET_WORDS))\n","\n","        # --- Try to match snippets to PubMed ---\n","        results = []\n","        for i, row in sample_df.iterrows():\n","            ok, pmid, how, code = _try_match(row[\"snippet\"], debug=(i < 5))  # debug first 5\n","            results.append({\n","                \"snippet\": row[\"snippet\"],\n","                \"label\": row[LABEL_COL],\n","                \"match\": bool(ok),\n","                \"pmid\": pmid,\n","                \"strategy\": how,\n","                \"http_status\": code\n","            })\n","            time.sleep(PUBMED_SLEEP)\n","\n","        # --- Summarize & display ---\n","        res_df = _round_df(pd.DataFrame(results))\n","        match_rate = float(res_df[\"match\"].mean() * 100.0) if len(res_df) else 0.0\n","        summary_df = _round_df(pd.DataFrame([{\n","            \"sample_n\": int(len(res_df)),\n","            \"matches\": int(res_df[\"match\"].sum()),\n","            \"match_rate_%\": round(match_rate, 3)\n","        }]))\n","\n","        print(\"\\nPubMed spot-check results (with debug on first 5):\")\n","        display(res_df)\n","        print(\"\\nPubMed spot-check summary:\")\n","        display(summary_df)\n","\n","        #exports\n","        res_df.to_csv(os.path.join(TABLE_DIR, \"pubmed_spotcheck_results.csv\"), index=False)\n","        summary_df.to_csv(os.path.join(TABLE_DIR, \"pubmed_spotcheck_summary.csv\"), index=False)"],"metadata":{"id":"vyE4cmPMzOYC","executionInfo":{"status":"ok","timestamp":1760832701590,"user_tz":-60,"elapsed":64,"user":{"displayName":"Idris Babalola","userId":"12686428794913987650"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["summary = (\n","    res_df.assign(kind=res_df.apply(lambda r: \"rate_limited_429\" if (r[\"http_status\"]==429 and not r[\"match\"])\n","                              else (\"true_nomatch\" if (r[\"http_status\"]==200 and not r[\"match\"])\n","                              else (\"matched\" if r[\"match\"] else \"other\")), axis=1))\n","      .groupby(\"kind\").size().rename(\"n\").reset_index()\n",")\n","summary[\"pct_%\"] = (summary[\"n\"] / len(res_df) * 100).round(3)\n","summary"],"metadata":{"id":"fZv-k84WzyQ0","executionInfo":{"status":"ok","timestamp":1760832711014,"user_tz":-60,"elapsed":33,"user":{"displayName":"Idris Babalola","userId":"12686428794913987650"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["# ====================================================================\n","# Diagnostic test for abstract labels: checking for diffusing level\n","# ====================================================================\n","\n","# ---------- working corpus (deduplicated) ----------\n","df_dedup = df_after.copy()\n","\n","def _tokenize_words(s: str):\n","    return re.findall(r\"[A-Za-z]+\", str(s).lower())\n","\n","def _row_norm(xr: csr_matrix) -> csr_matrix:\n","    denom = np.sqrt((xr.multiply(xr)).sum())\n","    return xr if denom == 0 else xr / denom\n","\n","\n","# ======================================================================\n","# 1) Cross-label duplicate membership by class (RAW df)\n","# ======================================================================\n","if \"cross_label\" not in globals():\n","    g_text = df.groupby(TEXT_COL)\n","    group_size = g_text.size().rename(\"group_size\")\n","    n_labels = g_text[LABEL_COL].nunique().rename(\"n_labels\")\n","    meta = pd.concat([group_size, n_labels], axis=1).reset_index()\n","    cross_label_local = meta[(meta[\"group_size\"] > 1) & (meta[\"n_labels\"] > 1)].copy()\n","else:\n","    cross_label_local = cross_label.copy()\n","\n","cross_texts = set(cross_label_local[TEXT_COL].tolist())\n","mask_cross = df[TEXT_COL].isin(cross_texts)\n","per_class_cross = (\n","    df.loc[mask_cross, LABEL_COL].value_counts()\n","      .rename_axis(\"Category\").to_frame(\"Count\").reset_index()\n",")\n","per_class_cross[\"Frac_in_crosslabel_%\"] = (per_class_cross[\"Count\"] / mask_cross.sum() * 100).round(3)\n","\n","display(_round_df(per_class_cross))\n","_round_df(per_class_cross).to_csv(os.path.join(TABLE_DIR, \"gpc_justification_crosslabel.csv\"), index=False)\n","\n","fig = plt.figure()\n","cats = per_class_cross[\"Category\"].tolist()\n","vals = per_class_cross[\"Count\"].tolist()\n","bars = plt.bar(range(len(cats)), vals)\n","plt.xticks(range(len(cats)), cats, rotation=30, ha='right')\n","plt.ylabel(\"Rows in cross-label duplicate groups\")\n","plt.title(\"Cross-label duplicate membership by class (RAW)\")\n","for i, b in enumerate(bars):\n","    h = b.get_height()\n","    pct = vals[i] / mask_cross.sum() * 100 if mask_cross.sum() else 0.0\n","    plt.text(b.get_x()+b.get_width()/2, h, f\"{int(h)} ({pct:.3f}%)\", ha='center', va='bottom', fontsize=9)\n","plt.grid(True, axis='y')\n","_show_and_save(fig, os.path.join(PLOT_DIR, \"crosslabel_membership_by_class.png\"))\n","\n","# ======================================================================\n","# 2) Pairwise Jaccard similarity (token overlap) on df_dedup\n","# ======================================================================\n","cat_tokens = {}\n","for cat, grp in df_dedup.groupby(LABEL_COL):\n","    toks = set()\n","    for t in grp[TEXT_COL]:\n","        toks.update(_tokenize_words(t))\n","    cat_tokens[cat] = toks\n","\n","cats_sorted = sorted(cat_tokens.keys())\n","recs = []\n","for i in range(len(cats_sorted)):\n","    for j in range(i+1, len(cats_sorted)):\n","        c1, c2 = cats_sorted[i], cats_sorted[j]\n","        inter = len(cat_tokens[c1] & cat_tokens[c2])\n","        union = max(1, len(cat_tokens[c1] | cat_tokens[c2]))\n","        recs.append({\"Category_1\": c1, \"Category_2\": c2, \"Jaccard\": round(inter/union, 3)})\n","jacc_df = pd.DataFrame(recs).sort_values([\"Category_1\",\"Category_2\"]).reset_index(drop=True)\n","display(_round_df(jacc_df))\n","_round_df(jacc_df).to_csv(os.path.join(TABLE_DIR, \"jaccard_pairs_after_dedup.csv\"), index=False)\n","\n","# heatmap\n","mat = np.zeros((len(cats_sorted), len(cats_sorted)))\n","for i, ci in enumerate(cats_sorted):\n","    for j, cj in enumerate(cats_sorted):\n","        if i==j:\n","            mat[i,j] = 1.0\n","        else:\n","            a, b = (ci, cj) if ci < cj else (cj, ci)\n","            v = jacc_df[(jacc_df[\"Category_1\"]==a)&(jacc_df[\"Category_2\"]==b)][\"Jaccard\"].values\n","            mat[i,j] = float(v[0]) if len(v) else 0.0\n","\n","fig = plt.figure(figsize=(7.5,6.5))\n","plt.imshow(mat, cmap=\"YlGnBu\", vmin=0, vmax=1)\n","plt.title(\"Category Jaccard Similarity (deduplicated corpus)\")\n","plt.xticks(range(len(cats_sorted)), cats_sorted, rotation=45, ha='right')\n","plt.yticks(range(len(cats_sorted)), cats_sorted)\n","for i in range(len(cats_sorted)):\n","    for j in range(len(cats_sorted)):\n","        plt.text(j, i, f\"{mat[i,j]:.3f}\", ha='center', va='center', fontsize=7)\n","plt.colorbar(label=\"Jaccard\")\n","_show_and_save(fig, os.path.join(PLOT_DIR, \"jaccard_heatmap_after_dedup.png\"))\n","\n","# ======================================================================\n","# 3) Centroid margin separability (unsupervised, TF-IDF word 1–2)\n","# ======================================================================\n","vec = TfidfVectorizer(analyzer=\"word\", ngram_range=(1,2), min_df=2, max_features=100_000)\n","X = vec.fit_transform(df_dedup[TEXT_COL].astype(str))\n","y = df_dedup[LABEL_COL].astype(str).values\n","classes = sorted(np.unique(y))\n","\n","# Build L2-normalized class centroids in **dense** ndarray form\n","centroids = {}\n","for c in classes:\n","    rows = (y == c)\n","    if rows.sum() == 0:\n","        centroids[c] = None\n","        continue\n","    v = np.asarray(X[rows].mean(axis=0)).ravel() # mean -> ndarray (1D)\n","    nrm = np.linalg.norm(v)\n","    centroids[c] = v if nrm == 0 else v / nrm\n","\n","# Compute cosine similarities to each centroid; margin = top1 - top2\n","margins = []\n","X_csr = X.tocsr()\n","for i in range(X_csr.shape[0]):\n","    xv = X_csr[i].toarray().ravel()\n","    nrm = np.linalg.norm(xv)\n","    xv = xv if nrm == 0 else xv / nrm\n","    sims = []\n","    for c in classes:\n","        centroid = centroids[c]\n","        sim = float(np.dot(xv, centroid)) if centroid is not None else 0.0\n","        sims.append((c, sim))\n","    sims.sort(key=lambda t: t[1], reverse=True)\n","    top1 = sims[0][1]\n","    top2 = sims[1][1] if len(sims) > 1 else 0.0\n","    margins.append({\"Category\": y[i], \"margin\": round(top1 - top2, 3)})\n","\n","margin_df = pd.DataFrame(margins)\n","margin_stats = (\n","    margin_df.groupby(\"Category\")[\"margin\"]\n","             .agg([\"count\",\"mean\",\"median\",\"std\",\"min\",\"max\"])\n","             .reset_index()\n",")\n","display(_round_df(margin_stats))\n","_round_df(margin_stats).to_csv(os.path.join(TABLE_DIR, \"centroid_margin_stats.csv\"), index=False)\n","\n","# Boxplot\n","fig = plt.figure(figsize=(9.5,5.5))\n","order = margin_df.groupby(\"Category\")[\"margin\"].median().sort_values(ascending=False).index.tolist()\n","data  = [margin_df.loc[margin_df[\"Category\"]==c, \"margin\"].values for c in order]\n","plt.boxplot(data, labels=order, showmeans=True)\n","plt.xticks(rotation=30, ha='right')\n","plt.ylabel(\"Centroid margin (top1 - top2 cosine sim)\")\n","plt.title(\"Unsupervised separability by class (higher = clearer boundary)\")\n","plt.grid(True, axis='y')\n","_show_and_save(fig, os.path.join(PLOT_DIR, \"centroid_margin_boxplot_by_class.png\"))\n","\n","\n","# ======================================================================\n","# Lexical exclusivity via log-odds (Monroe et al.) — z>2 distinctive tokens\n","# ======================================================================\n","# --- Token counts per class\n","def word_counts_per_class(df_in, text_col, label_col):\n","    class_counts = defaultdict(Counter)\n","    total_tokens = {}\n","    docs_tokens = defaultdict(list)  # store token lists per doc for concentration\n","    for cat, grp in df_in.groupby(label_col):\n","        cnt = Counter()\n","        n_tokens = 0\n","        for txt in grp[text_col].astype(str):\n","            toks = _tokenize_words(txt)\n","            docs_tokens[cat].append(toks)\n","            cnt.update(toks)\n","            n_tokens += len(toks)\n","        class_counts[cat] = cnt\n","        total_tokens[cat] = n_tokens\n","    return class_counts, total_tokens, docs_tokens\n","\n","class_counts, total_tokens, docs_tokens = word_counts_per_class(df_dedup, TEXT_COL, LABEL_COL)\n","classes = sorted(class_counts.keys())\n","vocab = sorted({w for c in classes for w in class_counts[c]})\n","bg = Counter()\n","for c in classes: bg.update(class_counts[c])\n","\n","# Monroe et al. (2008) log-odds with informative Dirichlet prior\n","alpha = 0.01  # symmetric prior\n","\n","def _log_odds_z_for_class(target):\n","    n1 = sum(class_counts[target].values())\n","    n2 = sum((bg - class_counts[target]).values())\n","    out = {}\n","    for w in vocab:\n","        c1 = class_counts[target][w]\n","        c2 = bg[w] - c1\n","        p1 = (c1 + alpha) / (n1 + alpha * len(vocab))\n","        p2 = (c2 + alpha) / (n2 + alpha * len(vocab))\n","        var = (1.0 / (c1 + alpha)) + (1.0 / (c2 + alpha))\n","        # log-odds difference with tiny guards\n","        denom1 = max(1e-12, 1 - p1)\n","        denom2 = max(1e-12, 1 - p2)\n","        delta = np.log(p1 / denom1) - np.log(p2 / denom2)\n","        z = delta / np.sqrt(var)\n","        out[w] = z\n","    return out\n","\n","zs_by_class = {c: _log_odds_z_for_class(c) for c in classes}\n","\n","# --- Aggregate exclusivity: count tokens with z > 2 per class ---\n","exclusivity = []\n","for c in classes:\n","    zmap = _log_odds_z_for_class(c)\n","    distinctive = sum(1 for z in zmap.values() if z > 2.0)\n","    exclusivity.append({\"Category\": c, \"distinctive_token_count_z>2\": distinctive})\n","excl_df = pd.DataFrame(exclusivity).sort_values(\"distinctive_token_count_z>2\", ascending=False)\n"," # --- Display & export exclusivity table ---\n","display(_round_df(excl_df))\n","_round_df(excl_df).to_csv(os.path.join(TABLE_DIR, \"lexical_exclusivity_summary.csv\"), index=False)\n","\n","fig = plt.figure()\n","cats = excl_df[\"Category\"].tolist()\n","vals = excl_df[\"distinctive_token_count_z>2\"].tolist()\n","bars = plt.bar(range(len(cats)), vals)\n","plt.xticks(range(len(cats)), cats, rotation=30, ha='right')\n","plt.ylabel(\"Distinctive tokens (z-score > 2)\")\n","plt.title(\"Lexical exclusivity by class\")\n","for i, b in enumerate(bars):\n","    plt.text(b.get_x()+b.get_width()/2, b.get_height(), f\"{int(vals[i])}\", ha='center', va='bottom', fontsize=9)\n","plt.grid(True, axis='y')\n","_show_and_save(fig, os.path.join(PLOT_DIR, \"lexical_exclusivity_bar.png\"))\n","\n","# =============================\n","# 4) Distinctive types per 10k tokens (z > 2)\n","# =============================\n","rows = []\n","for c in classes:\n","    n_tokens = max(1, total_tokens[c])\n","    n_distinctive = sum(1 for _, z in zs_by_class[c].items() if z > 2.0)\n","    per10k = n_distinctive / n_tokens * 10_000.0\n","    rows.append({\n","        \"Category\": c,\n","        \"total_tokens\": int(n_tokens),\n","        \"distinctive_types_z>2\": int(n_distinctive),\n","        \"distinctive_types_per10k\": round(per10k, 3),\n","    })\n","\n","per10k_df = pd.DataFrame(rows).sort_values(\"distinctive_types_per10k\", ascending=False)\n","display(_round_df(per10k_df))\n","\n","# save table\n","_per10k_path = os.path.join(TABLE_DIR, \"lexical_exclusivity_per10k.csv\")\n","_round_df(per10k_df).to_csv(_per10k_path, index=False)\n","\n","# plot bar\n","fig = plt.figure()\n","cats = per10k_df[\"Category\"].tolist()\n","vals = per10k_df[\"distinctive_types_per10k\"].tolist()\n","bars = plt.bar(range(len(cats)), vals)\n","plt.xticks(range(len(cats)), cats, rotation=30, ha='right')\n","plt.ylabel(\"Distinctive types per 10k tokens (z > 2)\")\n","plt.title(\"Normalized lexical exclusivity by class\")\n","for i, b in enumerate(bars):\n","    h = b.get_height()\n","    plt.text(b.get_x()+b.get_width()/2, h, f\"{h:.3f}\", ha='center', va='bottom', fontsize=9)\n","plt.grid(True, axis='y')\n","_show_and_save(fig, os.path.join(PLOT_DIR, \"lexical_exclusivity_per10k_bar.png\"))\n","\n","# ===========================================\n","# 5. Cohesion & Distinctiveness Concentration\n","# ===========================================\n","# own-centroid cosine per doc\n","X_csr = X.tocsr()\n","own_sim = []\n","for i in range(X_csr.shape[0]):\n","    xv = X_csr[i].toarray().ravel()\n","    nrm = np.linalg.norm(xv)\n","    xv = xv if nrm == 0 else xv / nrm\n","    sim = float(np.dot(xv, centroids[y[i]]))\n","    own_sim.append({\"Category\": y[i], \"own_centroid_cosine\": sim})\n","\n","cohesion_df = pd.DataFrame(own_sim).groupby(\"Category\")[\"own_centroid_cosine\"]\\\n","    .agg([\"count\",\"mean\",\"median\",\"std\",\"min\",\"max\"]).reset_index()\n","display(_round_df(cohesion_df))\n","cohesion_path = os.path.join(TABLE_DIR, \"cohesion_by_class.csv\")\n","_round_df(cohesion_df).to_csv(cohesion_path, index=False)\n","\n","# plot cohesion (mean cosine)\n","fig = plt.figure()\n","order = cohesion_df.sort_values(\"mean\", ascending=False)[\"Category\"].tolist()\n","vals  = cohesion_df.set_index(\"Category\").loc[order, \"mean\"].values\n","bars = plt.bar(range(len(order)), vals)\n","plt.xticks(range(len(order)), order, rotation=30, ha='right')\n","plt.ylabel(\"Mean own-centroid cosine\")\n","plt.title(\"Intra-class cohesion (higher = more coherent)\")\n","for i, b in enumerate(bars):\n","    h = b.get_height()\n","    plt.text(b.get_x()+b.get_width()/2, h, f\"{h:.3f}\", ha='center', va='bottom', fontsize=9)\n","plt.grid(True, axis='y')\n","_show_and_save(fig, os.path.join(PLOT_DIR, \"cohesion_mean_bar.png\"))\n","\n","# =============================\n","# 6) Top-K most distinctive tokens per class via log-odds\n","# =============================\n","TOP_K = 500  # can tune (100–1000)\n","rows = []\n","for c in classes:\n","    zmap = zs_by_class[c]\n","    topk = {w for w,_ in sorted(zmap.items(), key=lambda t: t[1], reverse=True)[:TOP_K]}\n","    doc_fracs = []\n","    for toks in docs_tokens[c]:\n","        if len(toks) == 0:\n","            doc_fracs.append(0.0)\n","        else:\n","            doc_fracs.append(sum(1 for t in toks if t in topk) / len(toks))\n","    rows.append({\n","        \"Category\": c,\n","        \"TopK\": TOP_K,\n","        \"mean_fraction_topK\": np.mean(doc_fracs),\n","        \"median_fraction_topK\": np.median(doc_fracs),\n","    })\n","\n","conc_df = pd.DataFrame(rows)\n","conc_df[[\"mean_fraction_topK\",\"median_fraction_topK\"]] = conc_df[[\"mean_fraction_topK\",\"median_fraction_topK\"]]*100\n","conc_df = conc_df.sort_values(\"mean_fraction_topK\", ascending=False)\n","display(_round_df(conc_df))\n","conc_path = os.path.join(TABLE_DIR, \"distinctiveness_concentration.csv\")\n","_round_df(conc_df).to_csv(conc_path, index=False)\n","\n","# plot concentration (mean % of tokens in Top-K)\n","fig = plt.figure()\n","cats = conc_df[\"Category\"].tolist()\n","vals = conc_df[\"mean_fraction_topK\"].tolist()\n","bars = plt.bar(range(len(cats)), vals)\n","plt.xticks(range(len(cats)), cats, rotation=30, ha='right')\n","plt.ylabel(\"% tokens in class Top-K distinctive list\")\n","plt.title(f\"Distinctiveness concentration (Top-{TOP_K}) — higher = more focused\")\n","for i, b in enumerate(bars):\n","    h = b.get_height()\n","    plt.text(b.get_x()+b.get_width()/2, h, f\"{h:.3f}%\", ha='center', va='bottom', fontsize=9)\n","plt.grid(True, axis='y')\n","_show_and_save(fig, os.path.join(PLOT_DIR, \"distinctiveness_concentration_bar.png\"))\n","\n","print(SEP)\n","print(\"Saved:\")\n","print(\" -\", os.path.join(PLOT_DIR, \"lexical_exclusivity_per10k_bar.png\"))\n","print(\" -\", cohesion_path)\n","print(\" -\", conc_path)\n","print(\" -\", os.path.join(PLOT_DIR, \"cohesion_mean_bar.png\"))\n","print(\" -\", os.path.join(PLOT_DIR, \"distinctiveness_concentration_bar.png\"))\n","print(\"Tables/plots saved to result/ and displayed above.\")"],"metadata":{"id":"PB1eKyZ259ZJ","executionInfo":{"status":"ok","timestamp":1760832729599,"user_tz":-60,"elapsed":86,"user":{"displayName":"Idris Babalola","userId":"12686428794913987650"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# ========================================================================\n","# Drop the 'General Pathological Conditions' and save final class labels\n","# ========================================================================\n","\n","EXCLUDED_CLASS = \"General Pathological Conditions\"\n","\n","# Filter out GPC and keep only selected columns\n","df_4class = df_after[df_after[LABEL_COL] != EXCLUDED_CLASS].copy()\n","\n","# Select the three columns of interest\n","df_4class = df_4class[['Category', 'Medical_Abstract', 'Category_Name']]\n","\n","# Save to CSV\n","out_path = os.path.join(TABLE_DIR, \"deduplicated_medical_abstract.csv\")\n","df_4class.to_csv(out_path, index=False, encoding=\"utf-8\")\n","\n","print(f\"Saved 4-class deduplicated corpus: {out_path}\")\n","print(f\"Rows: {len(df_4class)} | Columns: {list(df_4class.columns)}\")"],"metadata":{"id":"-tQdWs_NRaka","executionInfo":{"status":"ok","timestamp":1760832779899,"user_tz":-60,"elapsed":59,"user":{"displayName":"Idris Babalola","userId":"12686428794913987650"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["# ========================================================================\n","# OPTIONAL: Save results(i.e Plots and tables) to google drive\n","# ========================================================================\n","\n","# Mount Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import os\n","import shutil\n","from pathlib import Path\n","from typing import Tuple\n","\n","# ---- CONFIGURE YOUR PATHS HERE ----\n","\n","# === SOURCE FOLDERS ===\n","CSV_SRC_DIR = '/content/result/tables'\n","PNG_SRC_DIR = '/content/result/plots'\n","\n","# Destination root in Drive\n","DRIVE_ROOT = '/content/drive/MyDrive/' # <------ Adjust filepath to your desired destination\n","CSV_DST_DIR = os.path.join(DRIVE_ROOT, '02_EDA_CSV')\n","PNG_DST_DIR = os.path.join(DRIVE_ROOT, '01_EDA_Plots')\n","\n","\n","def format_bytes(n: int) -> str:\n","    for unit in ['B','KB','MB','GB','TB']:\n","        if n < 1024:\n","            return f\"{n:.2f} {unit}\"\n","        n /= 1024\n","    return f\"{n:.2f} PB\"\n","\n","def copy_folder_contents(src: str, dst: str) -> Tuple[int, int]:\n","    \"\"\"\n","    Recursively copy the *contents* of src into destination.\n","    - Creates directories as needed\n","    - Overwrites files in destination if they already exist\n","    \"\"\"\n","    src = os.path.abspath(src)\n","    dst = os.path.abspath(dst)\n","\n","    if not os.path.exists(src):\n","        print(f\"Source not found: {src}\")\n","        return (0, 0)\n","\n","    os.makedirs(dst, exist_ok=True)\n","    files_copied = 0\n","    bytes_copied = 0\n","\n","    for dirpath, dirnames, filenames in os.walk(src):\n","        rel = os.path.relpath(dirpath, src)\n","        target_dir = dst if rel == '.' else os.path.join(dst, rel)\n","        os.makedirs(target_dir, exist_ok=True)\n","\n","        for fname in filenames:\n","            s = os.path.join(dirpath, fname)\n","            d = os.path.join(target_dir, fname)\n","\n","            # Ensure overwrite: if target exists as a file, remove it first\n","            if os.path.isfile(d):\n","                try:\n","                    os.remove(d)\n","                except Exception as e:\n","                    print(f\"Could not remove existing file before overwrite: {d} ({e})\")\n","\n","            # Copy with metadata (copy2); will overwrite when path is the full file target\n","            shutil.copy2(s, d)\n","            files_copied += 1\n","            try:\n","                bytes_copied += os.path.getsize(s)\n","            except Exception:\n","                pass\n","\n","    return (files_copied, bytes_copied)\n","\n","# Run copies\n","print(\"Ensuring destination folders exist on Drive...\")\n","os.makedirs(CSV_DST_DIR, exist_ok=True)\n","os.makedirs(PNG_DST_DIR, exist_ok=True)\n","\n","print(f\"\\n Copying CSV folder:\\n  SRC: {CSV_SRC_DIR}\\n  DST: {CSV_DST_DIR}\")\n","csv_count, csv_bytes = copy_folder_contents(CSV_SRC_DIR, CSV_DST_DIR)\n","print(f\"CSV copy complete: {csv_count} files, {format_bytes(csv_bytes)}\")\n","\n","print(f\"\\n Copying PNG folder:\\n  SRC: {PNG_SRC_DIR}\\n  DST: {PNG_DST_DIR}\")\n","png_count, png_bytes = copy_folder_contents(PNG_SRC_DIR, PNG_DST_DIR)\n","print(f\"PNG copy complete: {png_count} files, {format_bytes(png_bytes)}\")\n","\n","# Quick verification - show first few items\n","def preview_dir(p: str, max_items: int = 20):\n","    try:\n","        items = sorted(os.listdir(p))[:max_items]\n","        print(f\"\\n {p} (showing up to {max_items} items):\")\n","        for it in items:\n","            print(\"  -\", it)\n","    except Exception as e:\n","        print(f\" Could not list {p}: {e}\")\n","\n","preview_dir(CSV_DST_DIR)\n","preview_dir(PNG_DST_DIR)\n","\n","print(\"\\n Done. Drive now has up-to-date copies (overwritten where names matched).\")"],"metadata":{"id":"4hpTF4YQI_v6","executionInfo":{"status":"ok","timestamp":1760832834625,"user_tz":-60,"elapsed":70,"user":{"displayName":"Idris Babalola","userId":"12686428794913987650"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["# ========================================================================\n","# Retrieving Library versions for reproducibility\n","# ========================================================================\n","\n","import importlib\n","import pkg_resources\n","\n","# List of libraries you've imported\n","imported_libraries = [\n","    're', 'json', 'numpy', 'pandas', 'matplotlib', 'IPython',\n","    'unicodedata', 'requests', 'sklearn', 'scipy'\n","]\n","\n","# Dictionary to store versions\n","versioned_packages = {}\n","\n","for lib in imported_libraries:\n","    try:\n","        # Use pkg_resources to get the version if available\n","        version = pkg_resources.get_distribution(lib).version\n","        versioned_packages[lib] = version\n","    except Exception:\n","        try:\n","            # Fall back to importing and checking __version__\n","            module = importlib.import_module(lib)\n","            version = getattr(module, '__version__', None)\n","            if version:\n","                versioned_packages[lib] = version\n","        except Exception:\n","            pass\n","\n","# Create requirements-style lines\n","requirements_lines = [f\"{lib}=={version}\" for lib, version in sorted(versioned_packages.items())]\n","print(requirements_lines)\n","\n","# Write to a requirements.txt file\n","with open(\"requirements.txt\", \"w\") as f:\n","    f.write(\"\\n\".join(requirements_lines))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dbzNPZr1aBKx","executionInfo":{"status":"ok","timestamp":1760828562575,"user_tz":-60,"elapsed":372,"user":{"displayName":"Idris Babalola","userId":"12686428794913987650"}},"outputId":"93582d12-9685-494e-fa21-d31d97029981"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["['IPython==7.34.0', 'json==2.0.9', 'matplotlib==3.10.0', 'numpy==2.0.2', 'pandas==2.2.2', 're==2.2.1', 'requests==2.32.4', 'scipy==1.16.2', 'sklearn==1.6.1']\n"]}]}]}